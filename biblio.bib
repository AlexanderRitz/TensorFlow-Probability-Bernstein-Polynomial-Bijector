
@article{azzalini-LookDataOld-1990,
  title = {A {{Look}} at {{Some Data}} on the {{Old Faithful Geyser}}},
  author = {Azzalini, A. and Bowman, A. W.},
  date = {1990},
  journaltitle = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {39},
  pages = {357--365},
  publisher = {{[Wiley, Royal Statistical Society]}},
  issn = {0035-9254},
  doi = {10.2307/2347385},
  abstract = {An analysis of data on the duration times and waiting times for eruptions from the Old Faithful Geyser reveals an interesting time series structure. A tentative physical model, derived from Rinehart, is outlined and a corresponding first-order Markov chain examined. It is shown that a second-order models is necessary to explain the observed correlations in the data. A curious clustering effect is apparent in the autocorrelation function when plotted over a large range of lags. Similar patterns are observed in simulations from the fitted second-order model.},
  eprint = {2347385},
  eprinttype = {jstor},
  number = {3}
}

@article{dinh-DensityEstimationUsing-2017,
  ids = {dinh-Densityestimationusing-2016},
  title = {Density estimation using {{Real NVP}}},
  author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  date = {2017-02-27},
  url = {http://arxiv.org/abs/1605.08803},
  urldate = {2020-02-04},
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  archivePrefix = {arXiv},
  eprint = {1605.08803},
  eprinttype = {arxiv},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Dinh et al/Dinh et al - 2017 - Density estimation using Real NVP.pdf;/home/marcel/Zotero/storage/LMW5IRGH/1605.html;/home/marcel/Zotero/storage/T2QTB8QQ/1605.html},
  keywords = {_tablet_modified,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning,thesis_main_processed,thesis_main_processed.2},
  primaryClass = {cs, stat}
}

@article{farouki-BernsteinPolynomialBasis-2012,
  title = {The {{Bernstein}} polynomial basis: {{A}} centennial retrospective},
  shorttitle = {The {{Bernstein}} polynomial basis},
  author = {Farouki, Rida T.},
  date = {2012-08-01},
  journaltitle = {Computer Aided Geometric Design},
  shortjournal = {Comput. Aided Geom. Des.},
  volume = {29},
  pages = {379--419},
  issn = {0167-8396},
  doi = {10.1016/j.cagd.2012.03.001},
  url = {https://doi.org/10.1016/j.cagd.2012.03.001},
  urldate = {2020-06-06},
  abstract = {One hundred years after the introduction of the Bernstein polynomial basis, we survey the historical development and current state of theory, algorithms, and applications associated with this remarkable method of representing polynomials over finite domains. Originally introduced by Sergei Natanovich Bernstein to facilitate a constructive proof of the Weierstrass approximation theorem, the leisurely convergence rate of Bernstein polynomial approximations to continuous functions caused them to languish in obscurity, pending the advent of digital computers. With the desire to exploit the power of computers for geometric design applications, however, the Bernstein form began to enjoy widespread use as a versatile means of intuitively constructing and manipulating geometric shapes, spurring further development of basic theory, simple and efficient recursive algorithms, recognition of its excellent numerical stability properties, and an increasing diversification of its repertoire of applications. This survey provides a brief historical perspective on the evolution of the Bernstein polynomial basis, and a synopsis of the current state of associated algorithms and applications.},
  file = {/home/marcel/Zotero/storage/GNU55SWR/Farouki - 2012 - The Bernstein polynomial basis A centennial retro.pdf},
  keywords = {Bernstein basis,Bézier curves and surfaces,Numerical stability,Polynomial algorithms,Polynomial approximation,thesis_main_processed,Weierstrass theorem},
  number = {6}
}

@article{hothorn-MostLikelyTransformations-2018,
  title = {Most {{Likely Transformations}}},
  author = {Hothorn, Torsten and Möst, Lisa and Bühlmann, Peter},
  date = {2018},
  journaltitle = {Scandinavian Journal of Statistics},
  volume = {45},
  pages = {110--134},
  issn = {1467-9469},
  doi = {10.1111/sjos.12291},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/sjos.12291},
  urldate = {2020-04-20},
  abstract = {We propose and study properties of maximum likelihood estimators in the class of conditional transformation models. Based on a suitable explicit parameterization of the unconditional or conditional transformation function, we establish a cascade of increasingly complex transformation models that can be estimated, compared and analysed in the maximum likelihood framework. Models for the unconditional or conditional distribution function of any univariate response variable can be set up and estimated in the same theoretical and computational framework simply by choosing an appropriate transformation function and parameterization thereof. The ability to evaluate the distribution function directly allows us to estimate models based on the exact likelihood, especially in the presence of random censoring or truncation. For discrete and continuous responses, we establish the asymptotic normality of the proposed estimators. A reference software implementation of maximum likelihood-based estimation for conditional transformation models that allows the same flexibility as the theory developed here was employed to illustrate the wide range of possible applications.},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Hothorn et al/Hothorn et al - 2018 - Most Likely Transformations.pdf;/home/marcel/Zotero/storage/ZGH8GBWI/sjos.html},
  keywords = {censoring,conditional distribution function,conditional quantile function,distribution regression,thesis_main_processed,thesis_main_processed.2,transformation model,truncation},
  langid = {english},
  number = {1}
}

@article{kingma-GlowGenerativeFlow-2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  shorttitle = {Glow},
  author = {Kingma, Diederik P. and Dhariwal, Prafulla},
  date = {2018-07-10},
  url = {http://arxiv.org/abs/1807.03039},
  urldate = {2020-02-04},
  abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow},
  archivePrefix = {arXiv},
  eprint = {1807.03039},
  eprinttype = {arxiv},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Kingma, Dhariwal/Kingma, Dhariwal - 2018 - Glow.pdf;/home/marcel/Zotero/storage/NCU9H4TL/1807.html},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,thesis_main_processed,thesis_main_processed.2},
  primaryClass = {cs, stat}
}

@article{kingma-ImprovingVariationalInference-2017,
  title = {Improving {{Variational Inference}} with {{Inverse Autoregressive Flow}}},
  author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  date = {2017-01-30},
  url = {http://arxiv.org/abs/1606.04934},
  urldate = {2020-02-21},
  abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
  archivePrefix = {arXiv},
  eprint = {1606.04934},
  eprinttype = {arxiv},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Kingma et al/Kingma et al - 2017 - Improving Variational Inference with Inverse Autoregressive Flow.pdf;/home/marcel/Zotero/storage/5DSUD62Q/1606.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,thesis_main_processed,thesis_main_processed.2},
  primaryClass = {cs, stat}
}

@article{mirza-ConditionalGenerativeAdversarial-2014,
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  date = {2014-11-06},
  url = {http://arxiv.org/abs/1411.1784},
  urldate = {2020-04-27},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  archivePrefix = {arXiv},
  eprint = {1411.1784},
  eprinttype = {arxiv},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Mirza,Osindero/Mirza,Osindero - 2014 - Conditional Generative Adversarial Nets.pdf;/home/marcel/Zotero/storage/65JCRNKZ/1411.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning,thesis_main_processed,thesis_main_processed.2},
  primaryClass = {cs, stat}
}

@article{oord-ParallelWaveNetFast-2017,
  title = {Parallel {{WaveNet}}: {{Fast High}}-{{Fidelity Speech Synthesis}}},
  shorttitle = {Parallel {{WaveNet}}},
  author = {van den Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and van den Driessche, George and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
  date = {2017-11-28},
  url = {http://arxiv.org/abs/1711.10433},
  urldate = {2019-12-02},
  abstract = {The recently-developed WaveNet architecture [27] is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today’s massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.},
  archivePrefix = {arXiv},
  eprint = {1711.10433},
  eprinttype = {arxiv},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Oord et al/Oord et al - 2017 - Parallel WaveNet.pdf},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,thesis_main_processed,thesis_main_processed.2},
  langid = {english},
  primaryClass = {cs}
}

@article{oord-WaveNetGenerativeModel-2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  date = {2016-09-19},
  url = {http://arxiv.org/abs/1609.03499},
  urldate = {2020-01-09},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archivePrefix = {arXiv},
  eprint = {1609.03499},
  eprinttype = {arxiv},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Oord et al/Oord et al - 2016 - WaveNet.pdf;/home/marcel/Zotero/storage/7M965KVV/1609.html},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Sound,proposal_processed,thesis_main_processed,thesis_main_processed.2},
  primaryClass = {cs}
}

@article{papamakarios-NormalizingFlowsProbabilistic-2019,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  date = {2019-12-05},
  url = {http://arxiv.org/abs/1912.02762},
  urldate = {2020-02-06},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1912.02762},
  eprinttype = {arxiv},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Papamakarios et al/Papamakarios et al - 2019 - Normalizing Flows for Probabilistic Modeling and Inference.pdf;/home/marcel/Zotero/storage/DTIJJQA9/1912.html},
  keywords = {_tablet_modified,Computer Science - Machine Learning,Statistics - Machine Learning,thesis_main_processed,thesis_main_processed.2},
  primaryClass = {cs, stat}
}

@article{rasul-MultivariateProbabilisticTime-2020,
  title = {Multi-variate {{Probabilistic Time Series Forecasting}} via {{Conditioned Normalizing Flows}}},
  author = {Rasul, Kashif and Sheikh, Abdul-Saboor and Schuster, Ingmar and Bergmann, Urs and Vollgraf, Roland},
  date = {2020-02-14},
  url = {http://arxiv.org/abs/2002.06103},
  urldate = {2020-02-17},
  abstract = {Time series forecasting is often fundamental to scientific and engineering problems and enables decision making. With ever increasing data set sizes, a trivial solution to scale up predictions is to assume independence between interacting time series. However, modeling statistical dependencies can improve accuracy and enable analysis of interaction effects. Deep learning methods are well suited for this problem, but multi-variate models often assume a simple parametric distribution and do not scale to high dimensions. In this work we model the multi-variate temporal dynamics of time series via an autoregressive deep learning model, where the data distribution is represented by a conditioned normalizing flow. This combination retains the power of autoregressive models, such as good performance in extrapolation into the future, with the flexibility of flows as a general purpose high-dimensional distribution model, while remaining computationally tractable. We show that it improves over the state-of-the-art for standard metrics on many real-world data sets with several thousand interacting time-series.},
  archivePrefix = {arXiv},
  eprint = {2002.06103},
  eprinttype = {arxiv},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Rasul et al/Rasul et al - 2020 - Multi-variate Probabilistic Time Series Forecasting via Conditioned Normalizing.pdf;/home/marcel/Zotero/storage/H9XBX8LD/2002.html},
  keywords = {Computer Science - Machine Learning,refference,Statistics - Machine Learning,thesis_main_processed,thesis_main_processed.2},
  primaryClass = {cs, stat}
}

@article{rezende-VariationalInferenceNormalizing-2016,
  ids = {rezende-VariationalInferenceNormalizing-2016a},
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  date = {2016-06-14},
  url = {http://arxiv.org/abs/1505.05770},
  urldate = {2020-02-04},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archivePrefix = {arXiv},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Rezende, Mohamed/Rezende, Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf;/home/marcel/Zotero/storage/PIGLZWXR/1505.html},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology,thesis_main_processed,thesis_main_processed.2},
  primaryClass = {cs, stat}
}

@article{rothfuss-NoiseRegularizationConditional-2020,
  title = {Noise {{Regularization}} for {{Conditional Density Estimation}}},
  author = {Rothfuss, Jonas and Ferreira, Fabio and Boehm, Simon and Walther, Simon and Ulrich, Maxim and Asfour, Tamim and Krause, Andreas},
  date = {2020-02-14},
  url = {http://arxiv.org/abs/1907.08982},
  urldate = {2020-04-09},
  abstract = {Modelling statistical relationships beyond the conditional mean is crucial in many settings. Conditional density estimation (CDE) aims to learn the full conditional probability density from data. Though highly expressive, neural network based CDE models can suffer from severe over-fitting when trained with the maximum likelihood objective. Due to the inherent structure of such models, classical regularization approaches in the parameter space are rendered ineffective. To address this issue, we develop a model-agnostic noise regularization method for CDE that adds random perturbations to the data during training. We demonstrate that the proposed approach corresponds to a smoothness regularization and prove its asymptotic consistency. In our experiments, noise regularization significantly and consistently outperforms other regularization methods across seven data sets and three CDE models. The effectiveness of noise regularization makes neural network based CDE the preferable method over previous non- and semi-parametric approaches, even when training data is scarce.},
  archivePrefix = {arXiv},
  eprint = {1907.08982},
  eprinttype = {arxiv},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Rothfuss et al/Rothfuss et al - 2020 - Noise Regularization for Conditional Density Estimation.pdf;/home/marcel/Zotero/storage/W7F6LVSM/1907.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,thesis_main_processed,thesis_main_processed.2},
  primaryClass = {cs, stat}
}

@article{sick-DeepTransformationModels-2020,
  title = {Deep transformation models: {{Tackling}} complex regression problems with neural network based transformation models},
  shorttitle = {Deep transformation models},
  author = {Sick, Beate and Hothorn, Torsten and Dürr, Oliver},
  date = {2020-04-01},
  url = {http://arxiv.org/abs/2004.00464},
  urldate = {2020-04-23},
  abstract = {We present a deep transformation model for probabilistic regression. Deep learning is known for outstandingly accurate predictions on complex data but in regression tasks, it is predominantly used to just predict a single number. This ignores the non-deterministic character of most tasks. Especially if crucial decisions are based on the predictions, like in medical applications, it is essential to quantify the prediction uncertainty. The presented deep learning transformation model estimates the whole conditional probability distribution, which is the most thorough way to capture uncertainty about the outcome. We combine ideas from a statistical transformation model (most likely transformation) with recent transformation models from deep learning (normalizing flows) to predict complex outcome distributions. The core of the method is a parameterized transformation function which can be trained with the usual maximum likelihood framework using gradient descent. The method can be combined with existing deep learning architectures. For small machine learning benchmark datasets, we report state of the art performance for most dataset and partly even outperform it. Our method works for complex input data, which we demonstrate by employing a CNN architecture on image data.},
  archivePrefix = {arXiv},
  eprint = {2004.00464},
  eprinttype = {arxiv},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Sick et al/Sick et al - 2020 - Deep transformation models.pdf;/home/marcel/Zotero/storage/5ZFRX2NW/2004.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,thesis_main_processed,thesis_main_processed.2},
  primaryClass = {cs, stat}
}

@inproceedings{sohn-LearningStructuredOutput-2015,
  title = {Learning structured output representation using deep conditional generative models},
  booktitle = {Advances in neural information processing systems 28},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  date = {2015},
  pages = {3483--3491},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf},
  file = {/home/marcel/Zotero/storage/3UXV9ESU/Sohn et al. - Learning Structured Output Representation using De.pdf},
  keywords = {thesis_main_processed,thesis_main_processed.2}
}

@article{tabak-FamilyNonparametricDensity-2013,
  title = {A {{Family}} of {{Nonparametric Density Estimation Algorithms}}},
  author = {Tabak, E. G. and Turner, Cristina V.},
  date = {2013},
  journaltitle = {Communications on Pure and Applied Mathematics},
  volume = {66},
  pages = {145--164},
  issn = {1097-0312},
  doi = {10.1002/cpa.21423},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21423},
  urldate = {2020-04-28},
  abstract = {A new methodology for density estimation is proposed. The methodology, which builds on the one developed by Tabak and Vanden-Eijnden, normalizes the data points through the composition of simple maps. The parameters of each map are determined through the maximization of a local quadratic approximation to the log-likelihood. Various candidates for the elementary maps of each step are proposed; criteria for choosing one includes robustness, computational simplicity, and good behavior in high-dimensional settings. A good choice is that of localized radial expansions, which depend on a single parameter: all the complexity of arbitrary, possibly convoluted probability densities can be built through the composition of such simple maps. © 2012 Wiley Periodicals, Inc.},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Tabak,Turner/Tabak,Turner - 2013 - A Family of Nonparametric Density Estimation Algorithms.pdf;/home/marcel/Zotero/storage/LIEP43AF/cpa.html},
  keywords = {thesis_main_processed,thesis_main_processed.2},
  langid = {english},
  number = {2}
}

@article{trippe-ConditionalDensityEstimation-2018,
  title = {Conditional {{Density Estimation}} with {{Bayesian Normalising Flows}}},
  author = {Trippe, Brian L. and Turner, Richard E.},
  date = {2018-02-13},
  url = {http://arxiv.org/abs/1802.04908},
  urldate = {2020-04-09},
  abstract = {Modeling complex conditional distributions is critical in a variety of settings. Despite a long tradition of research into conditional density estimation, current methods employ either simple parametric forms or are difficult to learn in practice. This paper employs normalising flows as a flexible likelihood model and presents an efficient method for fitting them to complex densities. These estimators must trade-off between modeling distributional complexity, functional complexity and heteroscedasticity without overfitting. We recognize these trade-offs as modeling decisions and develop a Bayesian framework for placing priors over these conditional density estimators using variational Bayesian neural networks. We evaluate this method on several small benchmark regression datasets, on some of which it obtains state of the art performance. Finally, we apply the method to two spatial density modeling tasks with over 1 million datapoints using the New York City yellow taxi dataset and the Chicago crime dataset.},
  archivePrefix = {arXiv},
  eprint = {1802.04908},
  eprinttype = {arxiv},
  file = {/home/marcel/Documents/eBooks/Zotero/Journal Article/Trippe,Turner/Trippe,Turner - 2018 - Conditional Density Estimation with Bayesian Normalising Flows.pdf;/home/marcel/Zotero/storage/G22GAVSY/1802.html},
  keywords = {Statistics - Machine Learning,thesis_main_processed,thesis_main_processed.2},
  primaryClass = {stat}
}

@article{weng-FlowbasedDeepGenerative-2018,
  title = {Flow-based deep generative models},
  author = {Weng, Lilian},
  date = {2018-10-13},
  journaltitle = {lilianweng.github.io/lil-log},
  url = {http://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html},
  keywords = {⛔ No DOI found,thesis_main_processed,thesis_main_processed.2}
}


